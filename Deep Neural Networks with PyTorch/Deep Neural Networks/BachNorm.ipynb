{"cells":[{"cell_type":"markdown","id":"efb1eb2d-b6b1-4a85-ae11-fc8d4b578489","metadata":{},"source":["<h1>Batch Normalization with the MNIST Dataset</h1>\n"]},{"cell_type":"markdown","id":"4c21bea3-a575-4a88-86f0-35e2c295b858","metadata":{},"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Define Several Neural Networks, Criterion function, Optimizer.</h5>\n","<h5> 2. Train Neural Network using Batch Normalization and no Batch Normalization </h5>   \n","\n"]},{"cell_type":"markdown","id":"4064ebfd-1c22-419d-9975-8223025849e8","metadata":{},"source":["<h2>Table of Contents</h2>\n","In this lab, you will build a Neural Network using Batch Normalization and compare it to a Neural Network that does not use Batch Normalization. You will use the MNIST dataset to test your network. \n","\n","<ul>\n","<li><a href=\"#Train_Func\">Neural Network Module and Training Function</a></li>\n","<li><a href=\"#Makeup_Data\">Load Data </a></li>\n","<li><a href=\"#NN\">Define Several Neural Networks, Criterion function, Optimizer</a></li>\n","<li><a href=\"#Train\">Train Neural Network using Batch Normalization and no Batch Normalization</a></li>\n","<li><a href=\"#Result\">Analyze Results</a></li>\n","</ul>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"2d15b2d1-bbb5-4e71-8c9e-1cf0d1bc4079","metadata":{},"source":["<h2>Preparation</h2>\n"]},{"cell_type":"markdown","id":"ba3a67ad-a04b-4c1d-9a9f-9902e536cbeb","metadata":{},"source":["We'll need the following libraries:  \n"]},{"cell_type":"code","execution_count":null,"id":"7785a878-4517-46a7-a3e3-e35f9498e1b7","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# These are the libraries will be used for this lab.\n","\n","# Using the following line code to install the torchvision library\n","# !mamba install -y torchvision\n","\n","import torch \n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","import torch.nn.functional as F\n","import matplotlib.pylab as plt\n","import numpy as np\n","torch.manual_seed(0)"]},{"cell_type":"markdown","id":"7690f9b9-1576-44c0-999b-697e7691b403","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"858da415-985b-48c2-8e39-98547f34e5b3","metadata":{},"source":["<h2 id=\"Train_Func\">Neural Network Module and Training Function</h2> \n"]},{"cell_type":"markdown","id":"2ab8cd7d-4d30-444e-8dfd-e2bfad07d0bf","metadata":{},"source":["Define the neural network module or class \n"]},{"cell_type":"markdown","id":"f91e33ef-920c-4912-ae1b-93e361a271d1","metadata":{},"source":[" Neural Network Module with two hidden layers using Batch Normalization\n"]},{"cell_type":"code","execution_count":null,"id":"8e8014c6-ae36-4ef0-aab0-e967c068bbd5","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Define the Neural Network Model using Batch Normalization\n","\n","class NetBatchNorm(nn.Module):\n","    \n","    # Constructor\n","    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n","        super(NetBatchNorm, self).__init__()\n","        self.linear1 = nn.Linear(in_size, n_hidden1)\n","        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n","        self.linear3 = nn.Linear(n_hidden2, out_size)\n","        self.bn1 = nn.BatchNorm1d(n_hidden1)\n","        self.bn2 = nn.BatchNorm1d(n_hidden2)\n","        \n","    # Prediction\n","    def forward(self, x):\n","        x = self.bn1(torch.sigmoid(self.linear1(x)))\n","        x = self.bn2(torch.sigmoid(self.linear2(x)))\n","        x = self.linear3(x)\n","        return x\n","    \n","    # Activations, to analyze results \n","    def activation(self, x):\n","        out = []\n","        z1 = self.bn1(self.linear1(x))\n","        out.append(z1.detach().numpy().reshape(-1))\n","        a1 = torch.sigmoid(z1)\n","        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n","        z2 = self.bn2(self.linear2(a1))\n","        out.append(z2.detach().numpy().reshape(-1))\n","        a2 = torch.sigmoid(z2)\n","        out.append(a2.detach().numpy().reshape(-1))\n","        return out"]},{"cell_type":"markdown","id":"f90e534a-ddff-408c-bfa7-fd2dc6aa3aa1","metadata":{},"source":["Neural Network Module with two hidden layers with out Batch Normalization\n"]},{"cell_type":"code","execution_count":null,"id":"9b6ef2b5-cd9b-4df2-9146-616354eae5ec","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Class Net for Neural Network Model\n","\n","class Net(nn.Module):\n","    \n","    # Constructor\n","    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n","\n","        super(Net, self).__init__()\n","        self.linear1 = nn.Linear(in_size, n_hidden1)\n","        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n","        self.linear3 = nn.Linear(n_hidden2, out_size)\n","    \n","    # Prediction\n","    def forward(self, x):\n","        x = torch.sigmoid(self.linear1(x))\n","        x = torch.sigmoid(self.linear2(x))\n","        x = self.linear3(x)\n","        return x\n","    \n","    # Activations, to analyze results \n","    def activation(self, x):\n","        out = []\n","        z1 = self.linear1(x)\n","        out.append(z1.detach().numpy().reshape(-1))\n","        a1 = torch.sigmoid(z1)\n","        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n","        z2 = self.linear2(a1)\n","        out.append(z2.detach().numpy().reshape(-1))\n","        a2 = torch.sigmoid(z2)\n","        out.append(a2.detach().numpy().reshape(-1))\n","        return out \n"]},{"cell_type":"markdown","id":"616cefa7-1ff8-4174-858f-e1ac3e046c07","metadata":{},"source":["Define a function to train the model. In this case the function returns a Python dictionary to store the training loss and accuracy on the validation data \n"]},{"cell_type":"code","execution_count":null,"id":"4573fb52-5231-407c-ad88-f10e8f811f55","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Define the function to train model\n","\n","def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n","    i = 0\n","    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n","\n","    for epoch in range(epochs):\n","        for i, (x, y) in enumerate(train_loader):\n","            model.train()\n","            optimizer.zero_grad()\n","            z = model(x.view(-1, 28 * 28))\n","            loss = criterion(z, y)\n","            loss.backward()\n","            optimizer.step()\n","            useful_stuff['training_loss'].append(loss.data.item())\n","            \n","        correct = 0\n","        for x, y in validation_loader:\n","            model.eval()\n","            yhat = model(x.view(-1, 28 * 28))\n","            _, label = torch.max(yhat, 1)\n","            correct += (label == y).sum().item()\n","            \n","        accuracy = 100 * (correct / len(validation_dataset))\n","        useful_stuff['validation_accuracy'].append(accuracy)\n","    \n","    return useful_stuff"]},{"cell_type":"markdown","id":"b20aca3f-aac3-4910-bda0-1219b9db7a0c","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"f28bdbc0-a5ac-4533-83dd-8c98f4165308","metadata":{},"source":["<h2 id=\"Makeup_Data\">Make Some Data</h2> \n"]},{"cell_type":"markdown","id":"25bee1b7-3827-40bb-84cc-95c5b4d1c977","metadata":{},"source":["Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code>\n"]},{"cell_type":"code","execution_count":null,"id":"ce5fa184-89cc-484f-9c88-99fdde4e6d31","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# load the train dataset\n","\n","train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"]},{"cell_type":"markdown","id":"6759e406-ffec-4e52-9096-0d1c4636d033","metadata":{},"source":["Load the validating dataset by setting the parameters train  <code>False</code> and convert it to a tensor by placing a transform object into the argument <code>transform</code>\n"]},{"cell_type":"code","execution_count":null,"id":"96a98682-2d3f-4938-a1a1-08dd29d1243b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# load the train dataset\n","\n","validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"]},{"cell_type":"markdown","id":"c4334066-97ec-420a-a7f0-3b31ed5762e9","metadata":{},"source":["create the training-data loader and the validation-data loader object \n"]},{"cell_type":"code","execution_count":null,"id":"72454719-4381-4e16-bb25-603021698809","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create Data Loader for both train and validating\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\n","validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)"]},{"cell_type":"markdown","id":"239c8314-dea7-4568-aa24-30ddde08e9ff","metadata":{},"source":["<a id=\"ref3\"></a>\n","<h2 align=center>Define Neural Network, Criterion function, Optimizer and Train the  Model  </h2> \n"]},{"cell_type":"markdown","id":"a77ce0fd-66f5-41b4-a990-720e24e08387","metadata":{},"source":["Create the criterion function  \n"]},{"cell_type":"code","execution_count":null,"id":"7c8a6ead-ef15-44e2-ae5b-03d5d9e39b23","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create the criterion function\n","\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"601a98e9-bad3-480f-aaf6-636911fd0d07","metadata":{},"source":["Variables for Neural Network Shape <code> hidden_dim</code> used for number of neurons in both hidden layers.\n"]},{"cell_type":"code","execution_count":null,"id":"89842332-c44e-4b1b-ba4d-f82f186c01eb","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Set the parameters\n","\n","input_dim = 28 * 28\n","hidden_dim = 100\n","output_dim = 10"]},{"cell_type":"markdown","id":"7bca10c0-8492-4922-8a59-94bf0b61a48c","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"7cc895df-807a-4973-adb9-05f02cfa457e","metadata":{},"source":["<h2 id=\"Train\">Train Neural Network using Batch Normalization and no Batch Normalization </h2> \n"]},{"cell_type":"markdown","id":"459a6c78-4efc-48f9-88a7-67593e18c2d0","metadata":{},"source":["Train Neural Network using  Batch Normalization :\n"]},{"cell_type":"code","execution_count":null,"id":"6021b78e-5a49-4267-8708-460dc53eba2c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create model, optimizer and train the model\n","\n","model_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim)\n","optimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1)\n","training_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5)"]},{"cell_type":"markdown","id":"6dab7dae-c541-427a-9134-43d949639940","metadata":{},"source":["Train Neural Network with no Batch Normalization:\n"]},{"cell_type":"code","execution_count":null,"id":"42066bf8-86b7-4dc3-9ee3-3341b1171908","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create model without Batch Normalization, optimizer and train the model\n","\n","model = Net(input_dim, hidden_dim, hidden_dim, output_dim)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n","training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5)"]},{"cell_type":"markdown","id":"b0b4fbfa-7fb5-4765-a13d-818604d8d41e","metadata":{},"source":["<h2 id=\"Result\">Analyze Results</h2> \n"]},{"cell_type":"markdown","id":"e6679408-b759-46da-99f6-a07e315f5612","metadata":{},"source":["Compare the histograms of the activation for the first layer of the first sample, for both models.\n"]},{"cell_type":"code","execution_count":null,"id":"66e2e111-da92-4462-83d9-d581ca005381","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model.eval()\n","model_norm.eval()\n","out=model.activation(validation_dataset[0][0].reshape(-1,28*28))\n","plt.hist(out[2],label='model with no batch normalization' )\n","out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))\n","plt.hist(out_norm[2],label='model with normalization')\n","plt.xlabel(\"activation \")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"f74fa26b-2ee2-40be-a34a-0673f7dc5ac3","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"06725d1b-4d78-42b6-a003-40e39402b2eb","metadata":{},"source":["We see the activations with Batch Normalization are zero centred and have a smaller variance.\n"]},{"cell_type":"markdown","id":"97ee1daf-abc6-4f39-af00-edf9cb7d968f","metadata":{},"source":["Compare the training loss for each iteration\n"]},{"cell_type":"code","execution_count":null,"id":"8f1af15c-4196-4c0b-bfe5-c00be1a363a7","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Plot the diagram to show the loss\n","\n","plt.plot(training_results['training_loss'], label='No Batch Normalization')\n","plt.plot(training_results_Norm['training_loss'], label='Batch Normalization')\n","plt.ylabel('Cost')\n","plt.xlabel('iterations ')   \n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"5d72abb7-4f93-4e35-9a0a-3e542122c955","metadata":{},"source":["Compare the validating accuracy for each iteration\n"]},{"cell_type":"code","execution_count":null,"id":"394a4b92-21cd-45d3-9f94-d5925b721483","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Plot the diagram to show the accuracy\n","\n","plt.plot(training_results['validation_accuracy'],label='No Batch Normalization')\n","plt.plot(training_results_Norm['validation_accuracy'],label='Batch Normalization')\n","plt.ylabel('validation accuracy')\n","plt.xlabel('epochs ')   \n","plt.legend()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
